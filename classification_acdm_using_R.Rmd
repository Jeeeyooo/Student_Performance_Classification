---
title: "acdm_classification"
author: "JEEYOO KIM"
date: "2018년 12월 14일"
output: html_document
---
# Student Academics Performance Data

#### R이 모델 성능 향상시키기에 편리한 몇몇 함수들이 있어서 사용해보도록 한다.

```{r}

library(caret)
library(irr)
library(C50)

df <-read.csv("your path\\clean_stdt_acdm.csv")

str(df)
df$esp<-factor(df$esp)
```

```{r}
set.seed(1234)
folds<-createFolds(df$esp, k=10)

cv_results <- lapply(folds, function(x){
  df_train <- df[-x,]
  df_test <- df[x, ]
  df_model <- C5.0(esp ~. , data = df_train)
  df_pred <- predict(df_model, df_test)
  df_actual <- df_test$esp
  torf = df_actual == df_pred
  accuracy = sum(torf)/length(torf)
  kappa <- kappa2(data.frame(df_actual, df_pred))$value
  #return(kappa)
  return(accuracy)
})

mean(unlist(cv_results))
```

DecisionTree - C5 : 67.54%


```{r}
set.seed(1234)
m <- train(esp ~ ., data = df, method = "C5.0")
m
```

59%

### C5.0에 대한 tuning
```{r}
ctrl <- trainControl(method = "cv", number = 10,
                     selectionFunction = "oneSE")


grid <- expand.grid(.model = "tree",
                    .trials = c(1, 5, 10, 15, 20),
                    .winnow = c("TRUE","FALSE"))


set.seed(1234)
m <- train(esp ~ ., data = df, method = "C5.0",
           metric = "Kappa",
           trControl = ctrl,
           tuneGrid = grid)
m
```

66.68%

#### Treebag
```{r}
library(ipred)
set.seed(1234)
ctrl <- trainControl(method = "cv", number = 10)
m<-train(esp ~ ., data = df, method = "treebag",
         trControl = ctrl)
m
```

accuracy : 68.12%
kappa : 0.5264 (moderate agreement)

#### adaboost
```{r}
library(adabag)
set.seed(1234)

adaboost_cv <- boosting.cv(esp ~ ., data = df)
1-adaboost_cv$error
```

63.36%


```{r}
adaboost_cv$confusion
library(vcd)
Kappa(adaboost_cv$confusion)
```


### regression
#### stepwise 
```{r}
df$esp <- as.numeric(df$esp)
full <- lm (esp ~ .,data = df)
summary(full)
null <- lm (esp ~ 1, data=df)
feature = step(data=df,null, direction = 'both', scope=list(upper=full))
f = toString(feature$call)
formula = substr(f, 5, nchar(f)-4)
```


####  변수 선택 후에 학습
```{r}
set.seed(1234)
folds<-createFolds(df$esp, k=10)



cv_results <- lapply(folds, function(x){
  df_train <- df[-x,]
  df_test <- df[x, ]

  df_model <- lm(formula, data=df_train)
  df_pred <- round(predict(df_model, df_test))
  df_actual <- df_test$esp
  torf = df_actual == df_pred
  accuracy = sum(torf)/length(torf)
  kappa <- kappa2(data.frame(df_actual, df_pred))$value
  #return(kappa)
  return(accuracy)
})
mean(unlist(cv_results))

```
stepwise regression accuracy : 68.36%


### 이 데이터는 변수 선택한 뒤 회귀분석 하는 것이 가장 나은 결과를 보였다.
